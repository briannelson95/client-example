"use strict";

Object.defineProperty(exports, "__esModule", {
  value: true
});
exports.observeFileAsset = observeFileAsset;
exports.observeImageAsset = observeImageAsset;
exports.uploadImageAsset = exports.uploadFileAsset = void 0;

var _operators = require("rxjs/operators");

var _rxjs = require("rxjs");

var _utils = require("../../utils");

const MAX_CONCURRENT_UPLOADS = 4;

function uploadSanityAsset(client, assetType, file) {
  let options = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : {};
  const extract = options.metadata;
  const preserveFilename = options.storeOriginalFilename;
  const {
    label,
    title,
    description,
    creditLine,
    source
  } = options;
  return hashFile(file).pipe((0, _operators.catchError)(() => // ignore if hashing fails for some reason
  (0, _rxjs.of)(null)), (0, _operators.mergeMap)(hash => // note: the sanity api will still dedupe unique files, but this saves us from uploading the asset file entirely
  hash ? fetchExisting(client, "sanity.".concat(assetType, "Asset"), hash) : (0, _rxjs.of)(null)), (0, _operators.mergeMap)(existing => {
    if (existing) {
      return (0, _rxjs.of)({
        // complete with the existing asset document
        type: 'complete',
        id: existing._id,
        asset: existing
      });
    }

    return client.observable.assets.upload(assetType, file, {
      tag: 'asset.upload',
      extract,
      preserveFilename,
      label,
      title,
      description,
      creditLine,
      source
    }).pipe((0, _operators.map)(event => event.type === 'response' ? {
      // rewrite to a 'complete' event
      type: 'complete',
      id: event.body.document._id,
      asset: event.body.document
    } : event));
  }));
}

const uploadAsset = (0, _utils.withMaxConcurrency)(uploadSanityAsset, MAX_CONCURRENT_UPLOADS);

const uploadImageAsset = (client, file, options) => uploadAsset(client, 'image', file, options);

exports.uploadImageAsset = uploadImageAsset;

const uploadFileAsset = (client, file, options) => uploadAsset(client, 'file', file, options); // note: there's currently 100% overlap between the ImageAsset document and the FileAsset documents as per interface required by the image and file input


exports.uploadFileAsset = uploadFileAsset;

function observeAssetDoc(documentPreviewStore, id) {
  return documentPreviewStore.observePaths({
    _type: 'reference',
    _ref: id
  }, ['originalFilename', 'url', 'metadata', 'label', 'title', 'description', 'creditLine', 'source', 'size']);
}

function observeImageAsset(documentPreviewStore, id) {
  return observeAssetDoc(documentPreviewStore, id);
}

function observeFileAsset(documentPreviewStore, id) {
  return observeAssetDoc(documentPreviewStore, id);
}

function fetchExisting(client, type, hash) {
  return client.observable.fetch('*[_type == $documentType && sha1hash == $hash][0]', {
    documentType: type,
    hash
  }, {
    tag: 'asset.find-duplicate'
  });
}

function readFile(file) {
  return new _rxjs.Observable(subscriber => {
    const reader = new FileReader();

    reader.onload = () => {
      subscriber.next(reader.result);
      subscriber.complete();
    };

    reader.onerror = err => {
      subscriber.error(err);
    };

    reader.readAsArrayBuffer(file);
    return () => {
      reader.abort();
    };
  });
}

function hashFile(file) {
  if (!window.crypto || !window.crypto.subtle || !window.FileReader) {
    return (0, _rxjs.of)(null);
  }

  return readFile(file).pipe((0, _operators.mergeMap)(arrayBuffer => crypto.subtle.digest('SHA-1', arrayBuffer)), (0, _operators.map)(hexFromBuffer));
}

function hexFromBuffer(buffer) {
  return Array.prototype.map.call(new Uint8Array(buffer), x => "00".concat(x.toString(16)).slice(-2)).join('');
}